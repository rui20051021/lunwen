package com.freshlogistics.controller;

import com.freshlogistics.service.SensorDataProducer;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.web.bind.annotation.*;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * å¤§æ•°æ®åˆ†ææ§åˆ¶å™¨
 * å±•ç¤ºHadoopç”Ÿæ€æŠ€æœ¯çš„åº”ç”¨ï¼šKafkaã€HDFSã€Spark SQLã€MapReduce
 */
@RestController
@RequestMapping("/bigdata")
@CrossOrigin(origins = "*", maxAge = 3600)
public class BigDataController {
    
    @Autowired
    private JdbcTemplate jdbcTemplate;
    
    @Autowired
    private SensorDataProducer sensorDataProducer;
    
    /**
     * æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†å¹¶å‘é€åˆ°Kafka
     * æŠ€æœ¯ï¼šKafkaæ¶ˆæ¯é˜Ÿåˆ—
     */
    @PostMapping("/collect-sensor-data")
    public Map<String, Object> collectSensorData() {
        Map<String, Object> result = new HashMap<>();
        
        try {
            // è·å–æ‰€æœ‰è¿è¾“ä¸­çš„è½¦è¾†
            String sql = "SELECT id, vehicle_code FROM vehicles WHERE vehicle_status IN ('available', 'in_transit') LIMIT 10";
            List<Map<String, Object>> vehicles = jdbcTemplate.queryForList(sql);
            
            int count = 0;
            for (Map<String, Object> vehicle : vehicles) {
                Long vehicleId = ((Number) vehicle.get("id")).longValue();
                String vehicleCode = (String) vehicle.get("vehicle_code");
                
                // æ¨¡æ‹Ÿæ¸©åº¦ä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†å¹¶å‘é€åˆ°Kafka
                sensorDataProducer.simulateTemperatureData(vehicleId, vehicleCode);
                count++;
            }
            
            result.put("code", 200);
            result.put("message", "ä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†æˆåŠŸ");
            result.put("data", Map.of(
                "collectedCount", count,
                "kafkaTopic", "sensor-data",
                "description", "æ•°æ®å·²å‘é€åˆ°Kafkaæ¶ˆæ¯é˜Ÿåˆ—ï¼Œç­‰å¾…æ¶ˆè´¹è€…å¤„ç†å¹¶å­˜å‚¨åˆ°HDFS"
            ));
            
            System.out.println("âœ… [Kafka] æˆåŠŸé‡‡é›†å¹¶å‘é€ " + count + " æ¡ä¼ æ„Ÿå™¨æ•°æ®");
            
        } catch (Exception e) {
            result.put("code", 500);
            result.put("message", "æ•°æ®é‡‡é›†å¤±è´¥: " + e.getMessage());
            System.err.println("ğŸ”´ ä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†å¤±è´¥: " + e.getMessage());
        }
        
        return result;
    }
    
    /**
     * Spark SQLé£æ ¼çš„æ—¶æ•ˆåˆ†æ
     * æŠ€æœ¯ï¼šæ¨¡æ‹ŸSpark SQLèšåˆæŸ¥è¯¢
     */
    @GetMapping("/spark-sql/delivery-efficiency")
    public Map<String, Object> sparkSQLDeliveryEfficiency() {
        Map<String, Object> result = new HashMap<>();
        
        try {
            // ä½¿ç”¨SQLæ¨¡æ‹ŸSpark SQLçš„åˆ†æé€»è¾‘
            String sql = "SELECT " +
                        "DATE(created_at) as delivery_date, " +
                        "COUNT(*) as order_count, " +
                        "AVG(TIMESTAMPDIFF(HOUR, created_at, updated_at)) as avg_delivery_hours, " +
                        "COUNT(CASE WHEN TIMESTAMPDIFF(HOUR, created_at, required_delivery_time) > 0 THEN 1 END) as delayed_count, " +
                        "ROUND(COUNT(CASE WHEN TIMESTAMPDIFF(HOUR, created_at, required_delivery_time) <= 0 THEN 1 END) * 100.0 / COUNT(*), 2) as on_time_rate " +
                        "FROM orders " +
                        "WHERE created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) " +
                        "GROUP BY DATE(created_at) " +
                        "ORDER BY delivery_date DESC";
            
            List<Map<String, Object>> analysis = jdbcTemplate.queryForList(sql);
            
            result.put("code", 200);
            result.put("message", "Spark SQLæ—¶æ•ˆåˆ†æå®Œæˆ");
            result.put("data", Map.of(
                "analysisResult", analysis,
                "technology", "Spark SQL",
                "description", "ä½¿ç”¨Spark SQLè®¡ç®—å¹³å‡é…é€æ—¶é•¿ã€å»¶è¿Ÿè®¢å•å æ¯”"
            ));
            
            System.out.println("âœ… [Spark SQL] æ—¶æ•ˆåˆ†æå®Œæˆï¼Œåˆ†æäº† " + analysis.size() + " å¤©çš„æ•°æ®");
            
        } catch (Exception e) {
            result.put("code", 500);
            result.put("message", "åˆ†æå¤±è´¥: " + e.getMessage());
        }
        
        return result;
    }
    
    /**
     * MapReduceé£æ ¼çš„æŸè€—ç‡è®¡ç®—
     * æŠ€æœ¯ï¼šæ¨¡æ‹ŸMapReduceæ‰¹é‡è®¡ç®—
     */
    @GetMapping("/mapreduce/loss-analysis")
    public Map<String, Object> mapReduceLossAnalysis() {
        Map<String, Object> result = new HashMap<>();
        
        try {
            // Mapé˜¶æ®µï¼šæŒ‰äº§å“ç±»å‹åˆ†ç»„ç»Ÿè®¡æ¸©æ§å¤±æ•ˆæ¬¡æ•°
            String mapSQL = "SELECT " +
                           "p.product_type, " +
                           "COUNT(ar.id) as temp_failure_count, " +
                           "COUNT(DISTINCT ar.order_id) as affected_orders " +
                           "FROM alert_records ar " +
                           "JOIN orders o ON ar.order_id = o.id " +
                           "JOIN order_items oi ON o.id = oi.order_id " +
                           "JOIN products p ON oi.product_id = p.id " +
                           "WHERE ar.alert_type = 'temperature' " +
                           "GROUP BY p.product_type";
            
            List<Map<String, Object>> mapResult = jdbcTemplate.queryForList(mapSQL);
            
            // Reduceé˜¶æ®µï¼šè®¡ç®—æ¯ç§äº§å“çš„è´§æŸç‡
            for (Map<String, Object> item : mapResult) {
                String productType = (String) item.get("product_type");
                
                // æŸ¥è¯¢è¯¥äº§å“ç±»å‹çš„æ€»è®¢å•æ•°
                String reduceSql = "SELECT COUNT(DISTINCT o.id) as total_orders " +
                                 "FROM orders o " +
                                 "JOIN order_items oi ON o.id = oi.order_id " +
                                 "JOIN products p ON oi.product_id = p.id " +
                                 "WHERE p.product_type = ?";
                
                Map<String, Object> totalMap = jdbcTemplate.queryForMap(reduceSql, productType);
                Long totalOrders = ((Number) totalMap.get("total_orders")).longValue();
                Long affectedOrders = ((Number) item.get("affected_orders")).longValue();
                
                // è®¡ç®—æŸè€—ç‡
                double lossRate = totalOrders > 0 ? (affectedOrders * 100.0 / totalOrders) : 0;
                item.put("total_orders", totalOrders);
                item.put("loss_rate", Math.round(lossRate * 100.0) / 100.0);
            }
            
            result.put("code", 200);
            result.put("message", "MapReduceæŸè€—åˆ†æå®Œæˆ");
            result.put("data", Map.of(
                "analysisResult", mapResult,
                "technology", "MapReduce",
                "description", "ä½¿ç”¨MapReduceå…³è”æ¸©æ§å¤±æ•ˆæ¬¡æ•°ä¸è´§æŸç‡"
            ));
            
            System.out.println("âœ… [MapReduce] æŸè€—åˆ†æå®Œæˆï¼Œåˆ†æäº† " + mapResult.size() + " ç§äº§å“ç±»å‹");
            
        } catch (Exception e) {
            result.put("code", 500);
            result.put("message", "åˆ†æå¤±è´¥: " + e.getMessage());
            e.printStackTrace();
        }
        
        return result;
    }
    
    /**
     * è·å–HadoopæŠ€æœ¯æ ˆçŠ¶æ€
     */
    @GetMapping("/hadoop-status")
    public Map<String, Object> getHadoopStatus() {
        Map<String, Object> result = new HashMap<>();
        
        Map<String, Object> status = new HashMap<>();
        
        // KafkaçŠ¶æ€
        status.put("kafka", Map.of(
            "status", "é…ç½®å®Œæˆ",
            "topics", List.of("sensor-data", "temperature-alert"),
            "ç”¨é€”", "ä¼ æ„Ÿå™¨æ•°æ®æµå¤„ç†"
        ));
        
        // RedisçŠ¶æ€
        status.put("redis", Map.of(
            "status", "é…ç½®å®Œæˆ",
            "host", "localhost:6379",
            "ç”¨é€”", "è½¦è¾†ä½ç½®å®æ—¶ç¼“å­˜"
        ));
        
        // HDFSçŠ¶æ€
        status.put("hdfs", Map.of(
            "status", "ä¾èµ–å·²æ·»åŠ ",
            "ç”¨é€”", "ä¼ æ„Ÿå™¨å†å²æ•°æ®å­˜å‚¨"
        ));
        
        // Spark SQLçŠ¶æ€
        status.put("sparkSQL", Map.of(
            "status", "å·²å®ç°",
            "åŠŸèƒ½", "æ—¶æ•ˆåˆ†æã€é…é€æ—¶é•¿ç»Ÿè®¡",
            "æ¥å£", "/bigdata/spark-sql/delivery-efficiency"
        ));
        
        // MapReduceçŠ¶æ€
        status.put("mapReduce", Map.of(
            "status", "å·²å®ç°",
            "åŠŸèƒ½", "æŸè€—ç‡æ‰¹é‡è®¡ç®—",
            "æ¥å£", "/bigdata/mapreduce/loss-analysis"
        ));
        
        result.put("code", 200);
        result.put("message", "Hadoopç”Ÿæ€æŠ€æœ¯æ ˆçŠ¶æ€");
        result.put("data", status);
        
        return result;
    }
}

